# 注意的点
	1. 任何代码，自己写完后，需要在脑海中模拟进行一遍CR(很重要)
	2. 即使是一道简单的题，如硬币找零，如果能写出抽象的写法，就尽量用抽象的写法。
	3.
# 个人想法
	如果做实时的推荐：可以拿到用户最近浏览的itemid list，然后将itemid的embedding特征通过sum pooling的方式，然后再和其他的做相似度计算。
# 快手
一个算法面试题：最近的公共父节点
gbdt节点分裂算法：贪心算法
als的评估方式：
离线：精确率指标达到一定的阈值
在线：userbased在线上AB的结果收益没有太大，所以没有写其他的。
als的梯度下降方式



# 小米
无算法面试题
需要关注：
1、推荐的数据量,7天70w。20天220w。
2、auc指标的计算方式，除了理论，实现是如何实现的，auc指标的意义
3、spark的数据不平衡如何解决。
4、下采样的意义，下采样会导致特定的指标过高，如何解决。
	- 参考网址:https://tracholar.github.io/machine-learning/2018/01/26/auc.html#auc%E5%AF%B9%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%AF%94%E4%BE%8B%E4%B8%8D%E6%95%8F%E6%84%9F ,会高估准确率和F1
参考样本


# 滴滴 
还了解哪些normalization方法:layer normalize 和 batch normalize
# 百度
是否了解word2vec

# boss直聘
spark数据不均衡

# 新氧APP
1. rmse为什么是96%，写错，应为0.96
2. SVM的几个核
3. 有没有尝试做过面积和价格的交叉特征，如

# 58同城
1. 协同过滤适合租房场景吗？1、不适合，只有一俩天的行为，而ALS协同本身是基于行为的
2. 冷启动：物品冷启动和用户冷启动问题。
3. 低阶函数拟合高阶

# 360金融
1. 等频离散化和等宽离散化的优缺点
	- 等宽：容易受噪声点的影响
		- 个人理解：等宽离散化如果把大部分特征都放入到同一个区间，说明该特征的区分度比较低(因为该特征的方差低,假设一个特征离散后为3个特征，分别为A、B、C,假设都集中集中在A，那么A的方差低，而B和C较少，B和C的方差也低，不适合模型的学习)
	- 等频: 容易把相同的值分到不同的桶中，(对于连续值这个异常不常见)
	
2. 自适应方法(adagrad adam)对稀疏特征和连续特征的作用

3. 下采样的方式

   - 最大点击位置以上，和最大点击位置以下的topk个

4. GBDT和xgboost对于缺失值的处理方式

  - 参考网址: https://blog.csdn.net/zhaomengszu/article/details/80775554
  - GBDT，将缺失值的样本放入到每个子节点中，具体参考西瓜书

5. query的kmeans聚类，会提前把小区的名称去掉后进行聚类.

6. 基于矩阵分解的协同过滤和基于近邻协同过滤的区别
	- 参考网址： https://www.jianshu.com/p/d15ba37755d1
	- 参考网址: https://www.docin.com/p-1128007752.html
	* 数据稀疏性。一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户
	* 注意，als交替最小二乘法，相比于SVD，没有svd分解矩阵的稳定性
	* 算法扩展性。最近邻居算法的计算量随着用户和物品数量的增加而增加，不适合数据量大的情况使用
# 微博
1. deep部分为什么可以泛化,deep可以学习到一个离散特征的embedding表示。embedding
	- deep部分输入的是连续特征和离散特征embedding后的特征
	- deep部门能学出特征的embedding表示
2. SVM和EM算法

3. 在第三面的时候，会给一道场景题，一个文章下面的推荐位推荐，会怎么做
	- 考虑是否需要商业化，可以前俩个槽位用模型,后俩个槽位用其他
	- 考虑冷启动问题
	- 标题之间相似度的问题，1. 挖掘同义词 2. tf-idf特征

# 腾讯面试


2. 有个问题，少量的item，还需要有召回吗




	- 新房每个item的个数较少，是否还需要走召回
	    我的想法是
	    1. 这是一套标准化的流程，即使item少，走召回同样可以减少线上需要使用的item
	    2. 不同的召回通道在没有办法走模型排序时，可以进行兜底，并且部分解决冷启动的问题
	    3. 有些场景只需要少量的推荐item时，可以尝试使用特定的召回通道进行推荐，而不用走模型。
3. 离线调研的GBDT模型，线上怎么使用
    我的想法是和之前客源挖掘的项目一样，每个客户推荐一个topk。

# 胜哥

	- 面试方法论
		1. 在回答一个问题，先要说出来，为什么会出现
		2. 解决了什么问题
		3. 在面试时，一定要观看面试官表情，如果面试官在仔细听，则认为可以，如果开始出现疑惑，则需要注意自己的表达
	- 面试的查漏补缺
		1. FM，因为会涉及到很多公司的技术选型，需要会推导
		2. xgboost是如何做特征的并行和如何做直方图近似的
		3. 因为面试的是推荐岗，需要关注
			- youtube的DSSM模型是如何做召回的
		4. 关注了LSTM后，还应该关注GRU
			传统的rnn无法解决长依赖的问题
			- 遗忘门
			- 输入门(作用，更新)
			- 单元状态，就是细胞状态
			- 输出门(作用，输出h(t))
			GRU:
			- 重置门
			- 更新门
	
		5. BP算法的公式推导
		6. word2vec是如何做算法加速的
		 	- 下采样方法的提速优化方法


# 阿里
	1. 为什么lstm可以解决梯度消失问题和学习到长依赖
		- 长依赖依赖于lstm的结构，细胞状态
		- sigmoid的导数是梯度消失的原因，全连接中的w是梯度爆炸的原因
		- LSTM可以解决梯度消失的问题的原因是从t-1时刻的细胞状态到t时刻的细胞状态
			- 参考网址:https://www.zhihu.com/question/34878706/answer/192444888
	
	2. LR和SVM的区别
		算法题: 判断一个list是否为排序二叉搜索树的后续遍历



# 腾讯
	1. 腾讯面试
		- item2vec适合什么样的场景  
			* 对于用户点击较少的场景不太适合
			* Item2vec对于热门item进行了采样并且对负样本也进行了采样从而使得其学习非热门item取得了效果。
			* 缺点: 1、只用到了item的共现信息，2、忽略了user行为序列信息 3、没有建模用户对不同item的喜欢程度高低
		- 阿里的din		- 需要弄清楚,深度兴趣网络使用的ele-wise的减,一个类似attention的机制
			* 参考网址:https://blog.csdn.net/caizd2009/article/details/87919952
			* 输入: 1. 用户画像特征
					2. 物品特征：由物品id，商店id，种类id构成。
					3. 候选广告集，也为物品，构成和物品特征一致
					4. 上下文特征。
			中间步骤：通过加法注意力的机制，然后将特征进行sum pooling


		- youtube的DSSM算法-召回			- 需要弄清楚
			* 分为了俩个阶段，第一个阶段预测u观看i的概率来学习embedding表示。
			* 参考网址: https://zhuanlan.zhihu.com/p/87452680
			* 对用户观看过的item做平均，求得用户播放行为的综合嵌入表示
			* 对用户的query也做相同方式的处理
			* 然后concat拼接了其他的非视频类的特征，如地址位置embedding和性别等
			* 需要理解DSSM中如何做用户embedding和item embedding
				* 在softmax之前的输入认为是user embedding,softmax之后的认为是item embedding
			* 使用内积度量寻找最相似的topN
		- DSSM算法- 排序
			
			* 和召回阶段不同，排序阶段对用户的播放时长建模，而不是预测点击率
			* 会相比排序阶段引入更多的特征
			* 使用了加权LR。参考网址: https://zhuanlan.zhihu.com/p/61827629
	
		- word2vec负采样  	- 需要弄清楚
		- 长尾的样本如何处理 	- 权值共享
			- 如果采用数据增强， 会对长尾的造成较大的噪声
			参考网址: 
		- 比如wide and deep ，为什么每层的神经单元个数是越来越少的。
		- 神经元的神经单元个数比之前小，会学习到压缩表示，如何减少这种信息丢失。
			* 根据胜哥的回答，下一层学习的都是上一层的更高级的表示，从dnn可以做交叉特征来理解，每个下一层是对上一层的更高级的表示，
			* 个人的回答是更上一层是
		- 其他的损失函数，例如NDCG，MAP，俩个表示排名的推荐指标
	
	2. 二面
		- 如何做一个开放式领域的问答系统
	3. 三面
		- 分布式排序，不会，有反问，说是有param serving的东西 
# 平安科技
	- userbased itembased的区别
		* userbased
			- 用于挖掘有共同兴趣的小团体，新颖性较高
			- 


​	

# vipkid
	一面：
	- 题目hard: 编辑距离
	- nlp题目: 需要了解Trie树
	二面：
	三面: 了解了我做的事情和希望我加入
	四面：
		有面试每个算法的特点、如GBDT+LR是如何进行特征组合的	
	五面:
		最大熵模型和极大似然估计的关系
			- 最大熵模型中极大对偶函数和极大似然等价(需要精准查看,参考李航统计方法学中的第87页)
		kmeans和EM算法的关系
			- kmeans中哪个部门是隐变量
			1. E步： 固定参数μkμk, 将每个数据点分配到距离它本身最近的一个簇类中
			2. M步：固定数据点的分配，更新参数（中心点）μkμk
			3. EM算法和K-Means算法的迭代过程比较类似，不同的是K-Means算法中每次对参数的更新是硬猜测，而EM中每次对参数的更新是软猜测；相同的是，两个算法都可能得到局部最优解，采用不同的初始参数迭代会有利于得到全局最优解
	
			链接：https://www.jianshu.com/p/2c42c567e893
			4. 这里的隐含类别变量指定方法比较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。
		EM算法的加深理解可以参考：EM算法的9层境界

# 头条
    - 一面:
        * spark写一下过程userbased的过程，讨论了spark
        * 硬币零钱问题
        * 硬币零钱问题升级版，target=16 list = 10 5 2,使用的回溯法
    
    - 二面:
    	项目经历
        lr
        cart树
        gbdt
        xgboost
        算法题1: 找到数组中大于target的最小连续的长度
        算法题2: leetcode 96题，不同的二叉树
# 阿里文娱
	- 写蓄水池算法,表现有点差

# 美团
	一面
	- 算法：LR、CNN、LSTM、GBDT、ID3、C4.5。ID3和C4.5的损失函数,注意ID3和C4.5只能用来做分类。ID3分支过程中总喜欢偏向取值较多的属性。ID3只能对分类变量进行处理，C4.5和cart树可以使用连续和分类俩种自变量
	
	- 快排、sql、，主要问遇到问题如何解决。
	- 问题：一面面试官问n取topk，我说用堆，然后又问用list可不可以，我说可以(因为堆本身就是用速度来实现的)，他说不行。。。
	ID3的损失函数：告诉我是条件熵。参考网址: https://blog.csdn.net/wzk1996/article/details/81941572,https://www.cnblogs.com/TimVerion/p/11211749.html.对于分类问题，损失函数实际上还是交叉熵
	二面
	- 推荐的业务，特征和模型在其他中的占比
	- 各种排序(堆排序、快排、归并排)的时间复杂度和空间复杂度
	- Trie树和线段树，
	- 场景题
BIGO
	一面：
		1. 给一个表uid，求uid出现次数的topk
		2. 给一个表uid,iid,将iid以uid为key转换成list
		3. 给定一个表uid,iid,rating，以uid为key，获取topk的rating



	二面：
		一个无序数组以o(n)的复杂度求排好序后的最大间隔，参考
		知道多臂老虎机吗，知道如何以boosting方法解决冷启动问题吗
		计算auc时，采用的是排序rank的方式，如果一个模型将某个正负样本的概率值预测为同一个概率，该如何处理
