# first
    未回答上的俩个问题：
    基于分数匹配的扩散和ddpm的基于噪声的扩散，一般都会有什么差别。
    GRPO会有什么缺点？
        从网上来看，就是需要针对单个prompt有多个response的回答
    
# 二面
    如果一个有多条路径都可以到达某个目标，但是有些目标中有一些敌人，应该如何设计reward？
    答：本身可以给一些相关的惩罚，比如遇到敌人，死亡惩罚的话，就会减少对这个的探索。
如果是开全图，可以使用每个人的位置构建一个基于距离的相关的能量场的方式，越近的危险程度越高。
    

    之前游戏AI中的寻路目标是固定，如果跟随的目标是移动的，这个时候如何设计较为合理的reward
    可以在特征中引入目标相对于你的移动速度、方法等信息。之类的是否更加靠近的reward在经过足够多的探索后，是可以学习到的.





