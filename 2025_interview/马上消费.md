# 为什么选择从普通风格转换为ip风格的方案，和直接写模仿风格有什么区别？
    答： 首先ip的风格不太好量化，如果直接以ip的形式直接放入prompt，其实数据量不多，希望能够有更多从更多的普通的风格中，跑出来更多的基于ip风格的数据

# reward的设计方法
    主要还是做reward的拆解。从稀疏的，往更加能够提升reward出现的相关概率的方向走


# 风格化的评测
    主要看场景和使用要求，看是不是只要风格化好就行，还是希望能有风格化的同时，能够更多的保留历史知识（也是当时引入kl散度的原因, 因为智能npc的使用场景，就是希望还可以进行相关的闲聊）


# 在RLHF中，ref的模型的产出结果概率分布，能否提前通过离线的方式计算好？





# 二面
    1. PRM（基于过程的监督）和ORM(基于结果的监督)，基于过程的监督是否会限制LLM的路径
    不会，reward是人设定的，只要人设定的reward能够引导模型从一个多方向去进行回答，比如一道算法题，本身就是有多种解法的，reward部分能够引导往多种方向去走，是ok的, 而且从R1给出的论文来看，语法格式<think>在最后计算的时候看起来是一个结果奖励（因为是根据产出的结果是否有相关token来计算的），但是实际上，think这个token词本身就有一个起到引导LLM进行思考的功能，所以从我的角度理解是一个过程奖励。而且<think>这个token会出现的比较早，从attention的角度来看，在实际上更多的起到一个过程奖励的作用，这其实从某种意义上可以看作是一种过程约束的设计方式，通过引入某些特定token因为过程的方式作为过程监督。
在LLM领域，我理解应该是一个过程奖励+结果监督奖励+多样性奖励。


    2. 预训练和SFT本质上都是基于next predict token的方式进行的训练，这俩个有什么区别？
    首先从一些业界的经验结果来看，预训练是学习知识的，在SFT的时候，一般是不学知识的，只是用来调整回答的分布更加符合人类。从这个loss来看，在sft中，prompt部分是不参与loss的计算的，只有response的部分参与，而prompt本质上就是一个很强的先验，假设一句话是100个词，然后经过token之后，是60个token，实际上，在计算这些出现概率的时候，相当于每个token的概率相乘，就已经很小了，所以SFT本质上相比于pre-train部分是一个带有很强的先验的情况下，做的一个loss的训练。
    
    * 如果做一些事情，会希望从哪些方向做？
    LLM+强化学习 做RLHF的微调，第二个好的场景是一个自动进化的agent的场景。比如有很多api，然后可以做很多相关的任务，如果给相关的reward后就可以看看是否能够自主进化。
    
    * deepseek有没有一些可以值的讲的技术上的？
    MLA和这个免loss设计的负载均衡，其实在训练过程中本质上是


    * 如何判断一个loss是否是好的
    1. 判断是否为稳定上升或者下降的（强化学习中reward需要稳定上升）
    2. 越到后面他的reward是否是更加接近一条直线的，即是否斜率为更加稳定下降的
    3. 如果reward在上升中后面突然猛烈上升的化，需要看有没有一些reward hacking的情况。
    4. 而在大模型的训练过程中，有可能会存在loss突变的过程，根据一些其他训练的经验来看，会把这部分的数据进行相关的移除进行train。

    
