# first
    在之前的强化学习中，是否遇到过reward hacking的情况，又是如何发现和如何解决？
    1. 有2种方式可以发现，第一种为通过监控查看，可以对比监督学习中的loss曲线，如果reward上升的趋势在后面的阶段没有变缓，而是一直上升，说明就是存在reward hacking; 第二种发现的方式就是看游戏里的表现
    在LLM中遇到类似的该怎么解决？
    
    # 如果存在比较合理的结果奖励reward的，可以将结果奖励reward放入，
    将reward首先进行相关的拆分, 比如准确性、安全性等，同时在进行标注的时候，标注的任务尽可能的简单，防止标注的要求太多从而方差较大
    # 如果采用DPO的方式，扩充了一些偏好的数据，但是没有取得好的效果，这个怎么办？
    首先从结果看，一定是标注的数据不太符合预期.可以从这几个方面查看
    和预训练一样，是否存在足够多样性，如果只是重复性的训练，实际上效果提升会比较的轻微
    查看标注的评判数据是否符合预期，可以评估下
    
    是否会弄一些point-wise loss和pair-wise loss相结合的
    本身个人理解强化学习就是基于pair-wise loss 做的，而在推荐里边也可以基于pair-wise的形式做，在clip中也是基于pair-wise的方式设计的loss
    leetcode:
    https://leetcode.cn/problems/longest-repeating-character-replacement/solutions/586648/ti-huan-hou-de-zui-chang-zhong-fu-zi-fu-eaacp/
