# 说明

1. 热点AI机器人中有什么相关的难点
    主要是学习es的过程中可能会踩坑，里边的默认分词器就是单个词进行分类的方式。第二个是需要调试prompt，有时候prompt并不符合预期，需要针对业务场景做一些prompt的精细化优化

2. 优化prompt有没有一些什么经验
    * 会做假定是xxx的角色，提升prompt专业度
    * 写的要求尽可能的少和明确，当要求过多时，会出现一些要求无法满足的情况
    * 对于上述俩个都无法满足的时候，可以额外通过其他的LLM或者其他方式收集相关的数据集进行SFT。

3. action和reward是如何设计，有没有一些业界是可以通用的一些经验
    reward设计时需要考虑最后的目的，然后进行拆解，比如游戏最后的胜利条件是占领棋子点，那么占领棋子点的reward就可以拆解部分出来，然后拆出一些其他的比如战斗类的reward等，
    action设计中，有一个trick，因为需要考虑更多的精细动作，所以在精细动作的细节上，会在角度较小的时候，设置更密集的action，强化学习中，为了减少探索，需要将

滑动窗口=k中找最大值
可以直接查看leetcode中滑动窗口的最大值




# 二面

主要咨询了项目和基于强化学习的直播场景题：
    有一个场景，本身的action会有互动xxx的，然后目标是总观播放时长和gmv，播放时长会受到推荐过来的人数的影响，这个如何建模。
    答：状态部分因为肯定能拿到本身已有的，然后action部分都是定好的，主要看reward部分能否做一些拆解，首先gmv等之类的reward可能是比较稀疏的，这个时候可以通过一些相关性方法判断是否存在其他的指标和这个gmv相关，这样可以让reward转换为一些更小的更稠密的reward。
    而总观均时长变化较大，所以是不太适合进行相关的学习的，看能否做一些处理，比如推荐进来的人从进来开始到离线的时长，做归一化，可以权重小点，而本身自来水这样的，reward大点之类的，减少推荐所带来的影响，同时可以看看是否有reward可以排除这个推荐进来的人群的影响。

算法题：
    1. 使用梯度下降的方式实现计算sqrt(n)

    2. 实现一个矩阵乘法, 不能用matmul
