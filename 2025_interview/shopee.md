# firtst

    有没有了解过一些大模型在LLM中的一些使用场景
    1. embedding接入，了解过embedding的一种训练方式，类似于bert中的这个CLS token，训练的时候使用的是这种交叉注意力的方式
    2. embedding 会和构建的目标相关，因为拿到的embedding是不变的，不像attention中能够根据seq的不同做一定的变化，而且embedding的获取方式不同也可能导致效果不同，不同层的网络拿到的信息是不同的。可以多拿几个层做一个尝试. 


# second

    概率题：以概率p 选择骰子A,A骰子以0.5的概率选择证明。
    以概率1-p选择骰子B，B为0.7的概率选择正面。

    求投掷N次，其中x次出现正面
