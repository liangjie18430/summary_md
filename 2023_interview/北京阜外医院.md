# 阜外医院 



1. 数据量多少，会不会产生这种过拟合的问题
 - 本身sft就是在过拟合一个场景
 - 而且sft的数据比较少，不会对大模型的指令遵循能力有较大的损坏，原本在没有做sft之前，大模型也是有这个能力做这个事的
 - 而且我们后边会有评测，如果评测的影响不大，是可以的，如果评测影响过大, 是可以通过gpt进行相关的数据集的扩充的。




2. 介绍一下推荐一个比医生更好的剂量这个场景如何使用ppo？

 - 首先需要定义怎么样是一个好的剂量
 - 需要判断这个剂量的生效是一个长期的还是一个短期的，如果是一个及时生效的，实际上是可以不需要使用ppo之类的强化学习算法
 - 是不是有大量的这个数据，如果有大量的样本，是可以考虑的
 - 然后还要看是不是有这个实时测试和收集数据的环境，如果没有的话，需要考虑一些off-policy的算法，不能考虑ppo之类的on-policy的算法


3. 会对ppo做一些创新吗
   一般都是做一些模型结构上的，而不是做一些loss上的改动


4. 如何解决推荐中的信息茧房的问题

 俩种方式，一种是去引入额外的外部数据，给用户扩充一些tag，这样感兴趣的就多了，可以推荐的东西就多了
   第二种是在推荐的过程中添加一些探索性的东西，这样就可以额外的捕捉用户的一些兴趣。


5. 推荐的时候是如何看指标的
    会有俩种指标，第一种在离线的时候，先看离线的auc之类的指标，然后在非离线的情况下，会通过AB实验的方式比较的关注一些额外的业务指标。
