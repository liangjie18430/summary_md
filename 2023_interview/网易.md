# 一面
业务方向： ACT游戏，雷火事业部，永劫无间游戏的对应的游戏agent
1.  问了ppo算法
2. 问了整体的训练流程架构
3. 问了ppo是如何保证老策略和新策略的

4. 问了dict是怎么实现的, 答hash
5. 问了虚函数？？？？

6. 跳、跑、下蹲等是用的一个action头还是多个action的头
答：有些动作是互斥的，所以会统一用一个相同的头，最后做一个policy action 到游戏里action的映射。

7. 游戏是多少帧的，训练是同步还是异步的

答：训练是同步训练的，在部署的时候，是异步，里边就会涉及到一个返回时间的问题，我们是要求2帧内进行返回。


leetcode题：hard

求二叉树中最大子路径和




# 二面

1. 问了在FPS的游戏AI中遇到了什么问题，怎么解决的

会遇到一些拟人化的问题，主要表现在规则占点时，会比较容易出现杀敌和去某一个点行为同时进行，比较刻板。
还有见面就进行杀敌等，这些都是一些拟人化的行为。

第二个是这个游戏机制的问题，因为游戏机制是一个占点类型的游戏。所以这个站点的行为优化比较难。

因为为了拟人性，所以最开始点规则占点，肯定是需要撤掉的。当时尝试了2种方案，一种为添加一个action，直接让模型预测需要去的点，本身3个点加上1个(空动作),通过可视化发现，这个方案在预测的时候, 会经常切换，这种实际上用处是不大的。

然后第二种方案，是移除了agent选择需要占点的规则，直接通过3个点整体占领得到的分数进行reward function的设计，让模型自动的选择去哪里。最后这个方案获得了不错的效果。




2. 训练是同一个策略，还是多个策略训练，怎么区分的多个职业？
是用的同一个网络进行训练的，里边主要通过相关的特征进行区分不同的职业做差异化，比如一些职业id，枪械id的不同，技能id的不同，护驾不同，基础数值不同等。


3. 会不会存在同质化的问题？
    在最开始没有做技能和使用规则的选择目的前，同质化是比较严重的，在二维图上看就能看出都是一出生就统一到某一个点占领，然后到下一个点占领的方式。
    

4. 为什么选了ddpg做看点的项目
答：保证不太影响大盘的效果下，使用小流量的试验，来验证强化学习是否可以对个性化推荐超参数起到作用.因为ddpg是off-policy的，所以可以从离线数据里训练得到一个较好的初始化的值.

4. 组织分工等
2正式2实习，实习生主要做一些function的设计和调优 
