# 一面
    - 在RLHF阶段，本身的learning部分和inference部分，时间上是不一致的，比如learning部分的loss计算，是并行的，但是inference是串行的
        * 参考3D并行的部分，类似于采用micro batch的形式，将inference按照layer 并行的方式部署，假设learn中的batch的大小为4，inference中，先接入第一条，inference完后，再inference第二条数据。最后一层的layer inference完后，可以发给learner。打一个时间差的方式进行。
    - 本身打reward人的水平可能不一样，如何构建合适的reward model, 因为本身人可能和更倾向于回答长的回答，如何解决这个问题
        * 打分的要求尽可能简单，减少不同人的方差
        * 构建一些其他规则的方式，来对答案进行评测
        * 还可以在相关的loss中，加入对response的不同长度的惩罚。
        
    - 问的面试官： 如何解决LLM的大模型问题
        * 扩大模型的scale
        * 使用一些搜索引擎，但是会带来新的问题，搜索引擎有可能解答的答案是错误的
        * 在给出答案的时候，给出引用的reference，虽然会提升可靠性，但是会降低可用性。
