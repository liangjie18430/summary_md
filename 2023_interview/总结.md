# 说明
注意：在面试的时候，如果面试官是那种比较希望你和他交流的类型，有想法的时候可以说出来，保持一个面试的节奏, 需要注意相应的沟通。如果是那种不在乎的人，可以不需要。


在一些非IDE上进行编程时，可以将相关的变量进行打印，来验证相关的逻辑


有效关注：有的点了关注后，后续会取消，这个就不是一个有效的关注




模型超时： 模型超时怎么解决？
  量化
  减少模型大小等
  做一些算子融合，比如模型压缩等
  优化取embedding的步骤，比如先去重，再取embedding
  知识蒸馏
  像传统的一样进行机器的扩容




排序深度不够的问题：
    如果简历中只写排序相关的，实际上可问的比较少。
    可以将特征重要性分析（sharpley和基于梯度的）；迁移学习(重点，对应minet等加入)，dpp混排， look-alike(类似于广告中)。



    迁移学习有2种方式，一种是直接挖掘可以带来正向收益的样本。第二个是类似minet那种，在广告领域即使使用其他的数据，同样还是会存在正样本过少的情况。



huanqin经验：
    id初始化添加side info，额外有一个全局的table（初始化的时候保证一定的区分性），然后进行attention计算选取里边的top_k,然后将user的特征+历史的t-1时刻的item，预测下一个t时刻的item_id，是一个自回归任务。(调试的时候可以先去掉topk进行测试)
   包括自己之前调试不可收敛的模型的时候，也是会慢慢的减少模型的结构


xiaoxi面试题：
   - 在一些场景，比如输入法中，因为涉及到一些内存使用和响应的问题，希望提示词尽可能的短。比如一个类似npc的翻译的任务，给定的prompt会有人物的背景或者风格设定之类的。如果给你只有对应的 "李云龙: query1", 这种，你该怎么做？
    * 个人的一些想法就是参考多模块中的方法，设定多个的query token，使用query token对提前设定的个人的一些背景知识和当前的query做一些attention，使得对应的query token 可以自适应的方式从原本设定的背景知识中attention出一些可以用于回答query的背景知识进行拼接到前面。loss也可以直接采用对应next token predict的相关的任务。(也有些类似于prefix-tuning的形式)。考虑到对于特定的问题，所给定的背景知识，token需要的数量也不一样，比如复杂的任务任务，可以设定使用更长的token前缀，简单的任务，比如只是问李云龙他出生在哪之类的问题的时候，实际上对应的prefix token可以没有那么长(如果有条件，可以做自适用的token个数的输出，如果做，需要思考)。在这种设定下，还可以减小对应的相关的响应时间。
  
