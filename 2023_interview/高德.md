# 一面




- 介绍下游戏相关的场景


- 监督学习和强化学习的差异
    个人觉得最大的差异是在整个框架上，一个考虑未来的收益，一个未考虑。在实现的时候，主要体现在loss相关的计算上。
    面试反馈说觉得最大的差异是强化学习有个reward model 的部分，可以弄出来很多的样本。个人反驳说，对比监督学习和强化学习差异的前提是他们的样本量一下，如果样本量不一样的前提下，哪个效果好的意义是不大的。
- 对推荐了解多少
    将了从召回到排序到混排部分
   算法：
    俩个字符串，A和B，假设A为"akdd47ak" B 为"ak47", 那么将B中的字符进行匹配，将匹配到的放前面，没有匹配的放后面


# 二面
    
    重点了解了下垂直频道相关的事情
    问有没有引入外部的点击的list
    有哪些tag
    
    算法：
        0-1背包问题

