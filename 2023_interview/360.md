# 一面
业务方向: 大模型安全
* 数据来源、如何评测、
    目的：目的是为了在垂直领域里
    实际上的整体流程是这样，会拿到原始的台本数据，然后构造一个prompt，调用gpt4，讲风格化的role的回答转换为中性的，然后通过人工标注的方式，选择更合适的中性风格句子，然后再进行了SFT。然后使用SFT的模型，针对我们的一个npc的使用场景，即拿到npc demo中的原始回答和一些常用的指令数据的原始回答，预测出2个回答，在发包标注任务的时候，会给标注人员一些参考的示例。针对相关的数据还需要进行一些抽检和回收。第二次发包会检查一致性和风格性.


翻译的prompt是： "你是一个语言助手，给定你一句台词，你需要将台词的风格转化为中性风格，但不能改变原有台词的意思",
    答： 一个偏翻译类型的任务，之前是有其他的调研是有俩种方式，一种为模仿直接回答，一种是先使用gpt进行回答，再通过一个sft出来的翻译模型进行翻译的方式。
    


* 大模型领域除了这个SFT之前，还有什么可以了解的吗？
 
从pretrain讲到(讲了7B和13B很多公司都能发布，实际上大的模型更多的是涉及到一些偏工程上的问题，简单是小模型的deepspeed是可以处理的)deepspeed，讲到基于MOE的模型，再讲到基于QFormer的相关的模型，讲第二步的指令微调，需要有更多的指令数据，根据第二步的情况，所以在做特定的SFT的时候，是需要进行相关的模型选型的。讲第三步的RLHF，其中的Reward Model是Setence Level的，但是在进行ppo计算的时候，实际上是加入了相关的每个token的kl散度的，对应的kl散度是咋token level的。   



* 给定一个场景，假设现在有100个npc，，如何选最具风格的三个role出来

先将数据转换为事实陈述的句子，比如将role的对应的answer，给定一些prompt，不能有语气词、脏话、骂人的话之类的，使用gpt4，将对应的句子弄成中性的句子。然后采用一些类似的距离函数，考虑到可能每个npc的句子数量不一定，可以做一定的归一化，然后排序进行选取。

选取的时候，根据之前的经验，相同的风格很可能比较难的细化出额外更细的npc出来，所以可以按照顺序的方式选择3个风格不同的role出来。

* 最近做的一个比较有成就感的事情
    强化学习里对agent的占领策略的建模，因为当前是没有相关的可以参考的reward function的设计的。
算法题：
    root到叶子节点的带权路径最大
    有什么优化的方向？ 能想到的是剪枝，但是还没有想到一些合理的剪枝的方式。  



