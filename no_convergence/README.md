# 说明
此处主要用于记录深度模型不收敛如何解决的问题
   

## 场景一
模型auc为0.5+,一直不收敛，有可能是模型深度太深


## 场景二
从其他经验中借鉴的，
现象就是auc 0.5+，而且收敛极慢，这应该是深度学习模型常见的问题。我们一步步打印向量，最后发现问题是MLP的输入值太小，跟其他向量不在一个量级。最后通过在MLP网络，每层加上Batch Normalization 和 Layer Normalization解决。没看懂和其他向量不在一个量级是什么意思。




## 场景三 
多兴趣向量之间差距非常小
现象就是，打印出来的多兴趣向量，向量之间的差异非常小，这样召回回来的item也没有太大差异。我们最后定位到问题是全局的routing_logits初始化的方差太小，导致之后每步生成的向量差距也不大，最后通过调大方差将问题解决。

这一版本比mind最初版，召回出来的结果的重复率从37.2%降低到32.7%，其他线上指标也有明显提升.
具体可以参考机器学习基础总结.doc中的记录
