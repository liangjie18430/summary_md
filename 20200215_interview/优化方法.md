# 梯度下降法
	- 每次使用所有样本的梯度
# 随机梯度下降
	- 每次使用单个
# mini-batch sgd
	- 每次选用一个batch_size进行训练


# 动量法
	- 定义 g(m) 为当前批次的梯度
	- v(m) = \beta*V(m-1)+g(m)
	- 更新w = w+v

# adagrad(自适应梯度)
	- 背景: 前面的算法都是使用全局的学习率，所有参数统一步伐进行更新。
	- 历史梯度的平方开根号，使用平方的原因是去除梯度符号的干扰

	* 缺点
		- 因为梯度很容易就会累积到一个很大的值，此时学习率就会被降低的很厉害，因此AdaGrad很容易过分的降低学习率率使其提前停止


# rmsprop(均方根传播)
	- 对过去和现在做一个均衡，在adgrad的基础上引入了衰减因子

	* 优点
	1. 相比于AdaGrad,这种方法很好的解决了深度学习中过早结束的问题
	2. 适合处理非平稳目标，对于RNN效果很好

	* 缺点
	1. 又引入了新的超参，衰减系数ρ

# adam (自适应动量优化)
	- 结合了动量法和均方根传播的方法，一阶和二阶都考虑了历史的衰减


# 牛顿法
	- 思想较为简单，直接使用二阶泰勒展开即可。



# 拟牛顿法
	- 思想：用不含二阶导的矩阵替代Hesse矩阵
	优点：1. 不用算二阶导数 2. 不用求逆
